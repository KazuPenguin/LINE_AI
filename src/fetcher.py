"""
ãƒ‡ãƒ¼ã‚¿åé›†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
Arxivã‹ã‚‰ã®è«–æ–‡åé›†ã¨DuckDuckGoã‚’ä½¿ã£ãŸWebæ¤œç´¢
"""

import arxiv
from datetime import datetime, timedelta
from ddgs import DDGS
from typing import List, Dict
import time


class DataFetcher:
    def __init__(self):
        self.ddgs = DDGS()
        self.days_back = 3

    def fetch_arxiv_papers(self) -> List[Dict]:
        """
        Arxivã‹ã‚‰éå»3æ—¥é–“ã®AI/LLMé–¢é€£è«–æ–‡ã‚’å–å¾—
        ã‚«ãƒ†ã‚´ãƒª: cs.CL, cs.AI, cs.LG, cs.CV, cs.SE
        """
        categories = ["cs.CL", "cs.AI", "cs.LG", "cs.CV", "cs.SE"]
        papers = []
        seen_ids = set()

        # éå»3æ—¥é–“ã®æ—¥ä»˜ã‚’è¨ˆç®—
        cutoff_date = datetime.now() - timedelta(days=self.days_back)

        print(f"ğŸ” Arxivè«–æ–‡ã‚’æ¤œç´¢ä¸­... (éå»{self.days_back}æ—¥é–“)")

        # arxiv Clientã‚’ä½¿ç”¨
        client = arxiv.Client()

        for category in categories:
            try:
                # å„ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰æ¤œç´¢
                search = arxiv.Search(
                    query=f"cat:{category}",
                    max_results=10,  # ä½™è£•ã‚’æŒã£ã¦å–å¾—
                    sort_by=arxiv.SortCriterion.SubmittedDate
                )

                count = 0
                for result in client.results(search):
                    # æå‡ºæ—¥ãŒ3æ—¥ä»¥å†…ã‹ãƒã‚§ãƒƒã‚¯
                    if result.published.replace(tzinfo=None) < cutoff_date:
                        continue

                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if result.entry_id in seen_ids:
                        continue

                    seen_ids.add(result.entry_id)
                    papers.append({
                        "title": result.title,
                        "abstract": result.summary,
                        "url": result.entry_id,
                        "published": result.published.strftime("%Y-%m-%d"),
                        "category": category
                    })

                    count += 1
                    if count >= 5:  # å„ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰æœ€å¤§5ä»¶
                        break

                print(f"  - {category}: {count}ä»¶å–å¾—")
                time.sleep(1)  # APIã«è² è·ã‚’ã‹ã‘ãªã„ã‚ˆã†å¾…æ©Ÿ

            except Exception as e:
                print(f"  âš ï¸ {category}ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        print(f"âœ… åˆè¨ˆ {len(papers)}ä»¶ã®è«–æ–‡ã‚’å–å¾—ã—ã¾ã—ãŸ\n")
        return papers

    def fetch_web_trends(self) -> List[Dict]:
        """
        DuckDuckGoã§AIé–¢é€£ã®æ–°ã‚µãƒ¼ãƒ“ã‚¹ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰æƒ…å ±ã‚’æ¤œç´¢
        """
        queries = [
            "AI new tool launch 2025",
            "LLM service announcement",
            "ChatGPT alternative new",
            "Generative AI product launch"
        ]

        results = []
        seen_urls = set()

        print("ğŸŒ Webä¸Šã®æ–°ã‚µãƒ¼ãƒ“ã‚¹ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’æ¤œç´¢ä¸­...")

        for query in queries:
            try:
                # DuckDuckGoæ¤œç´¢å®Ÿè¡Œ
                search_results = self.ddgs.text(
                    query,
                    max_results=3
                )

                for item in search_results:
                    if item["href"] in seen_urls:
                        continue

                    seen_urls.add(item["href"])
                    results.append({
                        "title": item["title"],
                        "snippet": item["body"],
                        "url": item["href"]
                    })

                time.sleep(2)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–

            except Exception as e:
                print(f"  âš ï¸ æ¤œç´¢ã‚¨ãƒ©ãƒ¼ ({query}): {e}")
                continue

        print(f"âœ… {len(results)}ä»¶ã®ãƒˆãƒ¬ãƒ³ãƒ‰æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ\n")
        return results[:10]  # æœ€å¤§10ä»¶

    def fetch_economic_news(self) -> List[Dict]:
        """
        AIæ¥­ç•Œã®çµŒæ¸ˆãƒ»ãƒ“ã‚¸ãƒã‚¹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ¤œç´¢
        """
        queries = [
            "OpenAI funding news 2025",
            "NVIDIA AI stock market",
            "AI startup investment",
            "Microsoft AI partnership"
        ]

        results = []
        seen_urls = set()

        print("ğŸ’° çµŒæ¸ˆãƒ»ãƒ“ã‚¸ãƒã‚¹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ¤œç´¢ä¸­...")

        for query in queries:
            try:
                search_results = self.ddgs.text(
                    query,
                    max_results=2
                )

                for item in search_results:
                    if item["href"] in seen_urls:
                        continue

                    seen_urls.add(item["href"])
                    results.append({
                        "title": item["title"],
                        "snippet": item["body"],
                        "url": item["href"]
                    })

                time.sleep(2)

            except Exception as e:
                print(f"  âš ï¸ æ¤œç´¢ã‚¨ãƒ©ãƒ¼ ({query}): {e}")
                continue

        print(f"âœ… {len(results)}ä»¶ã®çµŒæ¸ˆãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã—ã¾ã—ãŸ\n")
        return results[:8]  # æœ€å¤§8ä»¶

    def fetch_all(self) -> Dict:
        """
        ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã¦è¿”ã™
        """
        return {
            "papers": self.fetch_arxiv_papers(),
            "trends": self.fetch_web_trends(),
            "economic": self.fetch_economic_news()
        }


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    fetcher = DataFetcher()
    data = fetcher.fetch_all()

    print("\n=== å–å¾—çµæœã‚µãƒãƒªãƒ¼ ===")
    print(f"è«–æ–‡: {len(data['papers'])}ä»¶")
    print(f"ãƒˆãƒ¬ãƒ³ãƒ‰: {len(data['trends'])}ä»¶")
    print(f"çµŒæ¸ˆãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(data['economic'])}ä»¶")
